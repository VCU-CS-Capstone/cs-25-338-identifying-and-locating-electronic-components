{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn #related to neural networks\n",
    "import torch.optim as optim #optimizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms, models\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import numpy as np\n",
    "import sys\n",
    "import timm\n",
    "import tqdm\n",
    "from PIL import Image\n",
    "import json\n",
    "\n",
    "import matplotlib.patches as patches\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected image: images/train/Screenshot 2024-10-10 185220.png\n",
      "Annotations for selected image:\n",
      "[{'class_id': 35, 'polygon': [(359.00012499999997, 292.00005899999996), (423.99969500000003, 292.00005899999996), (423.99969500000003, 356.999985), (359.00012499999997, 356.999985)]}]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from glob import glob\n",
    "import yaml\n",
    "\n",
    "with open(\"data.yaml\", \"r\") as f:\n",
    "    data = yaml.safe_load(f)\n",
    "    \n",
    "# Define directories\n",
    "image_dir = \"images/train\"\n",
    "\n",
    "label_dir = \"labels/train\"\n",
    "\n",
    "# Get list of image files\n",
    "image_files = glob(os.path.join(image_dir, \"*.png\"))\n",
    "image_files.sort()\n",
    "\n",
    "# Load categories (if available)\n",
    "categories = [name for name in data[\"names\"]]\n",
    "\n",
    "# Select the 150th image (modify index as needed)\n",
    "x = 14\n",
    "selected_image_path = image_files[x]\n",
    "print(f\"Selected image: {selected_image_path}\")\n",
    "\n",
    "# Load the image\n",
    "image = cv2.imread(selected_image_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "img_height, img_width, _ = image.shape\n",
    "\n",
    "# Load corresponding annotations\n",
    "label_filename = os.path.splitext(os.path.basename(selected_image_path))[0] + \".txt\"\n",
    "label_path = os.path.join(label_dir, label_filename)\n",
    "\n",
    "annotations = []\n",
    "if os.path.exists(label_path):\n",
    "    with open(label_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) < 1 or (len(parts) - 1) % 2 != 0:\n",
    "                continue  # Skip invalid lines (odd number of coordinates)\n",
    "\n",
    "            class_id = int(parts[0])\n",
    "            polygon_norm = list(map(float, parts[1:]))  # Get all normalized coordinates\n",
    "            \n",
    "            # Convert normalized coordinates to absolute\n",
    "            polygon_abs = [\n",
    "                (x * img_width, y * img_height) \n",
    "                for x, y in zip(polygon_norm[::2], polygon_norm[1::2])\n",
    "            ]\n",
    "            \n",
    "            annotations.append({\n",
    "                'class_id': class_id,\n",
    "                'polygon': polygon_abs\n",
    "            })\n",
    "\n",
    "print(\"Annotations for selected image:\")\n",
    "print(annotations)\n",
    "\n",
    "# Plot the image with polygons\n",
    "fig, ax = plt.subplots(1, figsize=(12, 8))\n",
    "ax.imshow(image)\n",
    "\n",
    "for ann in annotations:\n",
    "    polygon = ann['polygon']\n",
    "    class_id = ann['class_id']\n",
    "    label = categories[class_id] if categories else str(class_id)\n",
    "    \n",
    "    # Draw polygon\n",
    "    poly_patch = patches.Polygon(\n",
    "        polygon,\n",
    "        closed=True,\n",
    "        linewidth=2,\n",
    "        edgecolor='red',\n",
    "        facecolor='none'\n",
    "    )\n",
    "    ax.add_patch(poly_patch)\n",
    "    \n",
    "    # Add label at the top-left corner of the polygon's bounding box\n",
    "    polygon_np = np.array(polygon)\n",
    "    x_min, y_min = polygon_np.min(axis=0)\n",
    "    plt.text(\n",
    "        x_min, y_min - 5,\n",
    "        label,\n",
    "        color='red',\n",
    "        fontsize=12,\n",
    "        bbox=dict(facecolor='white', alpha=0.5, edgecolor='none')\n",
    "    )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Circuit Diagram Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CircuitDiagramDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, transform=None, img_size=224):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "        self.img_size = img_size\n",
    "        \n",
    "        # Get list of image files\n",
    "        self.image_files = [f for f in os.listdir(image_dir) \n",
    "                          if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        original_width, original_height = image.size\n",
    "        \n",
    "        # Load corresponding YOLO annotations\n",
    "        label_name = os.path.splitext(img_name)[0] + \".txt\"\n",
    "        label_path = os.path.join(self.label_dir, label_name)\n",
    "        \n",
    "        boxes = []\n",
    "        labels = []\n",
    "        \n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) < 9:  # At least 1 class + 4 points (8 coordinates)\n",
    "                        continue\n",
    "                    \n",
    "                    # Parse YOLO polygon format\n",
    "                    class_id = int(parts[0])\n",
    "                    polygon_norm = list(map(float, parts[1:]))\n",
    "                    \n",
    "                    # Convert normalized polygon to absolute coordinates\n",
    "                    polygon_abs = [\n",
    "                        (x * original_width, y * original_height) \n",
    "                        for x, y in zip(polygon_norm[::2], polygon_norm[1::2])\n",
    "                    ]\n",
    "                    \n",
    "                    # Convert polygon to bounding box\n",
    "                    polygon_np = np.array(polygon_abs)\n",
    "                    x_min, y_min = polygon_np.min(axis=0)\n",
    "                    x_max, y_max = polygon_np.max(axis=0)\n",
    "                    \n",
    "                    # Convert to [x_min, y_min, x_max, y_max] format\n",
    "                    boxes.append([x_min, y_min, x_max, y_max])\n",
    "                    labels.append(class_id)\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        # Scale boxes to target image size\n",
    "        scale_x = self.img_size / original_width\n",
    "        scale_y = self.img_size / original_height\n",
    "        \n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "        if boxes.nelement() != 0:  # Check if boxes is not empty\n",
    "            boxes[:, 0::2] *= scale_x  # x coordinates\n",
    "            boxes[:, 1::2] *= scale_y  # y coordinates\n",
    "\n",
    "        labels = torch.tensor(labels, dtype=torch.int64)\n",
    "        \n",
    "        # Create target dictionary\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"image_id\": torch.tensor([idx])\n",
    "        }\n",
    "        \n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "dataset = CircuitDiagramDataset(\n",
    "    image_dir=\"images/train\",\n",
    "    label_dir=\"images/labels\",\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "def get_model(num_classes=40):\n",
    "    \"\"\"\n",
    "    Create a Faster R-CNN model with a ResNet-50-FPN backbone\n",
    "    Args:\n",
    "        num_classes (int): Number of object classes (excluding background)\n",
    "    \"\"\"\n",
    "    # 1. Load pre-trained base model\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(\n",
    "        weights='DEFAULT'  # Uses pre-trained weights\n",
    "    )\n",
    "    \n",
    "    # 2. Modify the classifier head\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    \n",
    "    # Important: num_classes + 1 (for background class)\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes + 1)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],  # ImageNet stats\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize dataset and dataloaders\n",
    "train_dataset = CircuitDiagramDataset(\"images/train\", \"labels/train\", transform=transform)\n",
    "val_dataset = CircuitDiagramDataset(\"images/val\", \"labels/val\", transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "# Initialize model\n",
    "num_classes = 40  # Adjust based on dataset\n",
    "model = get_model(num_classes)\n",
    "device = \"mps\"\n",
    "model.to(device)\n",
    "\n",
    "# Define optimizer and learning rate scheduler\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.AdamW(params, lr=1e-4, weight_decay=1e-4)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, train_loader, val_loader, device, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for images, targets in train_loader:\n",
    "            images = list(img.to(device) for img in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss_dict = model(images, targets)\n",
    "            loss = sum(loss for loss in loss_dict.values())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0\n",
    "            for images, targets in val_loader:\n",
    "                images = list(img.to(device) for img in images)\n",
    "                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "                loss_dict = model(images, targets)\n",
    "                loss = sum(loss for loss in loss_dict.values())\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        lr_scheduler.step()\n",
    "        print(f\"Epoch {epoch+1}, Train Loss: {total_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    print(\"Training Complete\")\n",
    "\n",
    "train_model(model, train_loader, val_loader, device, num_epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.83 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.68 ðŸš€ Python-3.12.3 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3060, 12288MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=/home/enerinn/cs-25-338-identifying-and-locating-electronic-components/Model/data.yaml, epochs=20, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=cuda, workers=4, project=None, name=train46, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=/home/enerinn/cs-25-338-identifying-and-locating-electronic-components/runs/detect/train46\n",
      "Overriding model.yaml nc=80 with nc=47\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    760477  ultralytics.nn.modules.head.Detect           [47, [64, 128, 256]]          \n",
      "Model summary: 225 layers, 3,020,013 parameters, 3,019,997 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/enerinn/cs-25-338-identifying-and-locating-electronic-components/Model/labels/train.cache... 3162 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3162/3162 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/enerinn/cs-25-338-identifying-and-locating-electronic-components/Model/labels/val.cache... 1054 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1054/1054 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to /home/enerinn/cs-25-338-identifying-and-locating-electronic-components/runs/detect/train46/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000196, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1m/home/enerinn/cs-25-338-identifying-and-locating-electronic-components/runs/detect/train46\u001b[0m\n",
      "Starting training for 20 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/20     0.436G      1.782      5.579      1.306         17        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [00:36<00:00,  5.48it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:10<00:00,  3.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1054       1055      0.487      0.216      0.154      0.101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/20      2.26G      1.531      4.043      1.185         13        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [00:28<00:00,  7.04it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:06<00:00,  4.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1054       1055      0.504      0.475      0.446       0.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/20      2.25G      1.479      3.355      1.168         17        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [00:28<00:00,  6.95it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:06<00:00,  5.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1054       1055      0.476      0.666      0.647      0.413\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/20      2.25G      1.431        2.8      1.151         12        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [00:27<00:00,  7.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:07<00:00,  4.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1054       1055      0.717      0.771      0.811       0.51\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/20      2.26G      1.387      2.395       1.13         14        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [00:27<00:00,  7.25it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:06<00:00,  5.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1054       1055      0.792      0.844      0.883       0.58\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/20      2.25G      1.389      2.131      1.122         19        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [00:27<00:00,  7.19it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:06<00:00,  4.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1054       1055      0.879      0.885      0.947      0.616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/20      2.26G      1.349      1.912      1.113         12        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [00:29<00:00,  6.76it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:07<00:00,  4.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1054       1055      0.886      0.864      0.944      0.622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/20      2.25G      1.337      1.789      1.104         22        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [00:29<00:00,  6.80it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:06<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1054       1055      0.908      0.939      0.968      0.644\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/20      2.26G      1.315      1.643      1.101         18        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [00:29<00:00,  6.73it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:07<00:00,  4.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1054       1055      0.941      0.956       0.98      0.654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/20      2.25G       1.31      1.559        1.1         18        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [00:29<00:00,  6.73it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:07<00:00,  4.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1054       1055      0.931       0.96      0.982      0.665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/20      2.26G      1.219      1.422      1.094         10        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [00:29<00:00,  6.73it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:07<00:00,  4.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1054       1055      0.937      0.952      0.976      0.654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/20      2.25G      1.204      1.318      1.089         10        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [00:29<00:00,  6.81it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:07<00:00,  4.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1054       1055      0.945      0.965      0.981      0.671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/20      2.26G      1.191      1.235      1.082         10        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [00:30<00:00,  6.39it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:08<00:00,  4.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1054       1055      0.957      0.976      0.987      0.664\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/20      2.25G      1.187      1.188      1.076         10        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [00:31<00:00,  6.37it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:07<00:00,  4.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1054       1055      0.953      0.964      0.979      0.669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/20      2.26G      1.188      1.145      1.073         10        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [00:28<00:00,  6.85it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:06<00:00,  4.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1054       1055      0.968      0.971      0.988      0.676\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/20      2.25G      1.149      1.094      1.064         10        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [00:28<00:00,  6.87it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:06<00:00,  4.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1054       1055      0.969       0.97      0.985      0.674\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/20      2.26G      1.157       1.08      1.061         10        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [00:28<00:00,  6.89it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:06<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1054       1055      0.957      0.985      0.987      0.685\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/20      2.25G      1.151      1.043      1.063         10        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [00:29<00:00,  6.75it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:06<00:00,  4.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1054       1055      0.959      0.981      0.986      0.683\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      19/20      2.26G      1.128      1.025      1.052         10        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [00:29<00:00,  6.76it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:06<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1054       1055      0.971      0.983      0.989      0.685\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      20/20      2.25G      1.128      1.015       1.05         10        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198/198 [00:28<00:00,  6.85it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:07<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1054       1055      0.968      0.986       0.99      0.688\n",
      "\n",
      "20 epochs completed in 0.206 hours.\n",
      "Optimizer stripped from /home/enerinn/cs-25-338-identifying-and-locating-electronic-components/runs/detect/train46/weights/last.pt, 6.3MB\n",
      "Optimizer stripped from /home/enerinn/cs-25-338-identifying-and-locating-electronic-components/runs/detect/train46/weights/best.pt, 6.3MB\n",
      "\n",
      "Validating /home/enerinn/cs-25-338-identifying-and-locating-electronic-components/runs/detect/train46/weights/best.pt...\n",
      "Ultralytics 8.3.68 ðŸš€ Python-3.12.3 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3060, 12288MiB)\n",
      "Model summary (fused): 168 layers, 3,014,813 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:09<00:00,  3.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1054       1055      0.968      0.986       0.99      0.688\n",
      "                  25FP          7          7       0.96          1      0.995      0.721\n",
      "                  30FP          7          7      0.969          1      0.995      0.756\n",
      "                  35FP          7          7       0.96          1      0.995      0.824\n",
      "            38BRKR_ATT         12         12      0.969          1      0.995       0.81\n",
      "                  40FP          6          6      0.661          1      0.995       0.87\n",
      "                  45FP          7          7      0.951          1      0.995      0.898\n",
      "                50FPUP         26         26      0.985          1      0.995      0.833\n",
      "                AIRGAP         26         26          1      0.973      0.995      0.622\n",
      "                 ARROW         26         26      0.986          1      0.995      0.629\n",
      "              AUTOXFMR         26         26      0.989          1      0.995      0.822\n",
      "              BRKR_ATT         28         28      0.985          1      0.995      0.664\n",
      "                   CAP         23         23      0.991          1      0.995      0.572\n",
      "                 CKTSW         23         23       0.99          1      0.995      0.622\n",
      "                CONNPT         14         14      0.898          1      0.995      0.656\n",
      "                    CT         28         28       0.99          1      0.995      0.708\n",
      "                CTASSY         25         25      0.985          1      0.995      0.748\n",
      "                   DDE         26         26      0.991          1      0.995      0.724\n",
      "               DIPPOLE         28         28      0.988          1      0.995      0.754\n",
      "                 FRGNP          7          7      0.955          1      0.995      0.661\n",
      "                FUCLSP         25         25       0.99          1      0.995      0.687\n",
      "               FUCOMGO         27         27      0.998          1      0.995      0.633\n",
      "                   GEN          7          7       0.75          1      0.964      0.773\n",
      "                  GEN1         26         26      0.986          1      0.995      0.795\n",
      "                   GND         24         24          1      0.997      0.995      0.462\n",
      "                 GNDSW         26         26      0.991          1      0.995      0.676\n",
      "                  GOBS         24         24      0.985          1      0.995      0.871\n",
      "                JUMPER         28         28      0.994          1      0.995      0.526\n",
      "                   LDE         20         20      0.986          1      0.995      0.493\n",
      "              LIGHTARR         26         26          1      0.999      0.995      0.661\n",
      "               LTERMLB         26         26      0.987          1      0.995      0.678\n",
      "              LTERMNLB         28         28      0.987          1      0.995      0.591\n",
      "             Meter_Dev         28         29      0.982      0.966      0.967      0.657\n",
      "                  MOCS         26         26          1      0.937      0.995       0.56\n",
      "                   MOS         25         25      0.961          1      0.986      0.497\n",
      "               POT_DEV         28         28      0.986          1      0.995      0.759\n",
      "                PTXFMR         24         24      0.948      0.958      0.943      0.612\n",
      "                   REG         28         28      0.989          1      0.995      0.741\n",
      "                   SBD         28         28      0.918      0.798      0.945      0.402\n",
      "                   SEC         25         25      0.989          1      0.995      0.728\n",
      "                   SPD         28         28      0.961      0.872      0.949      0.338\n",
      "              STA_SERV         24         24      0.986          1      0.995      0.759\n",
      "               SWFUCOM         26         26      0.988          1      0.995      0.629\n",
      "                  TPGO         24         24          1      0.857      0.992      0.552\n",
      "              WAVETRAP         27         27      0.987          1      0.995      0.848\n",
      "        WAVETRAP_NOPOT         24         24      0.988          1      0.995      0.877\n",
      "          WAVETRAP_POT         26         26      0.986          1      0.995      0.874\n",
      "                  XFMR         24         24      0.985          1      0.995      0.777\n",
      "Speed: 0.2ms preprocess, 1.7ms inference, 0.0ms loss, 1.6ms postprocess per image\n",
      "Results saved to \u001b[1m/home/enerinn/cs-25-338-identifying-and-locating-electronic-components/runs/detect/train46\u001b[0m\n",
      "Ultralytics 8.3.68 ðŸš€ Python-3.12.3 torch-2.6.0+cu124 CUDA:0 (NVIDIA GeForce RTX 3060, 12288MiB)\n",
      "Model summary (fused): 168 layers, 3,014,813 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/enerinn/cs-25-338-identifying-and-locating-electronic-components/Model/labels/val.cache... 1054 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1054/1054 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 66/66 [00:13<00:00,  5.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       1054       1055      0.968      0.987       0.99      0.689\n",
      "                  25FP          7          7      0.959          1      0.995      0.714\n",
      "                  30FP          7          7      0.968          1      0.995      0.777\n",
      "                  35FP          7          7      0.959          1      0.995      0.824\n",
      "            38BRKR_ATT         12         12      0.968          1      0.995       0.81\n",
      "                  40FP          6          6       0.66          1      0.995       0.87\n",
      "                  45FP          7          7       0.95          1      0.995      0.898\n",
      "                50FPUP         26         26      0.985          1      0.995      0.831\n",
      "                AIRGAP         26         26          1      0.974      0.995      0.627\n",
      "                 ARROW         26         26      0.986          1      0.995      0.633\n",
      "              AUTOXFMR         26         26      0.989          1      0.995      0.824\n",
      "              BRKR_ATT         28         28      0.985          1      0.995      0.668\n",
      "                   CAP         23         23      0.991          1      0.995      0.566\n",
      "                 CKTSW         23         23      0.989          1      0.995      0.626\n",
      "                CONNPT         14         14      0.897          1      0.995      0.648\n",
      "                    CT         28         28       0.99          1      0.995      0.708\n",
      "                CTASSY         25         25      0.985          1      0.995      0.755\n",
      "                   DDE         26         26      0.991          1      0.995      0.729\n",
      "               DIPPOLE         28         28      0.988          1      0.995      0.753\n",
      "                 FRGNP          7          7      0.955          1      0.995      0.661\n",
      "                FUCLSP         25         25       0.99          1      0.995      0.686\n",
      "               FUCOMGO         27         27      0.998          1      0.995      0.628\n",
      "                   GEN          7          7      0.731          1      0.964      0.751\n",
      "                  GEN1         26         26      0.986          1      0.995      0.784\n",
      "                   GND         24         24          1      0.998      0.995      0.461\n",
      "                 GNDSW         26         26      0.991          1      0.995      0.679\n",
      "                  GOBS         24         24      0.985          1      0.995      0.874\n",
      "                JUMPER         28         28      0.993          1      0.995      0.524\n",
      "                   LDE         20         20      0.986          1      0.995      0.496\n",
      "              LIGHTARR         26         26          1      0.999      0.995      0.665\n",
      "               LTERMLB         26         26      0.987          1      0.995      0.669\n",
      "              LTERMNLB         28         28      0.987          1      0.995      0.587\n",
      "             Meter_Dev         28         29      0.982      0.966      0.967      0.665\n",
      "                  MOCS         26         26          1      0.938      0.995       0.56\n",
      "                   MOS         25         25       0.96          1      0.984      0.508\n",
      "               POT_DEV         28         28      0.985          1      0.995      0.767\n",
      "                PTXFMR         24         24      0.948      0.958      0.943        0.6\n",
      "                   REG         28         28      0.989          1      0.995      0.739\n",
      "                   SBD         28         28      0.918      0.798      0.945      0.409\n",
      "                   SEC         25         25      0.988          1      0.995      0.753\n",
      "                   SPD         28         28      0.961      0.873      0.949      0.342\n",
      "              STA_SERV         24         24      0.986          1      0.995      0.765\n",
      "               SWFUCOM         26         26      0.988          1      0.995      0.634\n",
      "                  TPGO         24         24          1      0.871      0.992      0.546\n",
      "              WAVETRAP         27         27      0.987          1      0.995      0.848\n",
      "        WAVETRAP_NOPOT         24         24      0.988          1      0.995       0.88\n",
      "          WAVETRAP_POT         26         26      0.986          1      0.995      0.874\n",
      "                  XFMR         24         24      0.985          1      0.995       0.78\n",
      "Speed: 0.2ms preprocess, 6.8ms inference, 0.0ms loss, 1.3ms postprocess per image\n",
      "Results saved to \u001b[1m/home/enerinn/cs-25-338-identifying-and-locating-electronic-components/runs/detect/train462\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from ultralytics import YOLO\n",
    "\n",
    "torch.backends.cudnn.benchmark = True  # Auto-tune CUDA performance\n",
    "\n",
    "model = YOLO(\"yolov8n.pt\")  # Use a well-optimized YOLO model\n",
    "\n",
    "results = model.train(\n",
    "    data=\"/home/enerinn/cs-25-338-identifying-and-locating-electronic-components/Model/data.yaml\",\n",
    "    epochs=20,\n",
    "    batch=16,  # Adjust based on VRAM\n",
    "    device=\"cuda\",\n",
    "    workers=4,\n",
    "    amp=True,  # Mixed precision for Tensor Core speedup\n",
    "    imgsz=640  # Smaller image size speeds up training\n",
    ")\n",
    "\n",
    "\n",
    "metrics = model.val()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
