{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn #related to neural networks\n",
    "import torch.optim as optim #optimizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms, models\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import numpy as np\n",
    "import sys\n",
    "import timm\n",
    "import tqdm\n",
    "from PIL import Image\n",
    "import json\n",
    "\n",
    "import matplotlib.patches as patches\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected image: images/train/Screenshot 2024-10-10 185220.png\n",
      "Annotations for selected image:\n",
      "[{'class_id': 35, 'polygon': [(359.00012499999997, 292.00005899999996), (423.99969500000003, 292.00005899999996), (423.99969500000003, 356.999985), (359.00012499999997, 356.999985)]}]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from glob import glob\n",
    "import yaml\n",
    "\n",
    "with open(\"data.yaml\", \"r\") as f:\n",
    "    data = yaml.safe_load(f)\n",
    "    \n",
    "# Define directories\n",
    "image_dir = \"images/train\"\n",
    "\n",
    "label_dir = \"labels/train\"\n",
    "\n",
    "# Get list of image files\n",
    "image_files = glob(os.path.join(image_dir, \"*.png\"))\n",
    "image_files.sort()\n",
    "\n",
    "# Load categories (if available)\n",
    "categories = [name for name in data[\"names\"]]\n",
    "\n",
    "# Select the 150th image (modify index as needed)\n",
    "x = 14\n",
    "selected_image_path = image_files[x]\n",
    "print(f\"Selected image: {selected_image_path}\")\n",
    "\n",
    "# Load the image\n",
    "image = cv2.imread(selected_image_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "img_height, img_width, _ = image.shape\n",
    "\n",
    "# Load corresponding annotations\n",
    "label_filename = os.path.splitext(os.path.basename(selected_image_path))[0] + \".txt\"\n",
    "label_path = os.path.join(label_dir, label_filename)\n",
    "\n",
    "annotations = []\n",
    "if os.path.exists(label_path):\n",
    "    with open(label_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) < 1 or (len(parts) - 1) % 2 != 0:\n",
    "                continue  # Skip invalid lines (odd number of coordinates)\n",
    "\n",
    "            class_id = int(parts[0])\n",
    "            polygon_norm = list(map(float, parts[1:]))  # Get all normalized coordinates\n",
    "            \n",
    "            # Convert normalized coordinates to absolute\n",
    "            polygon_abs = [\n",
    "                (x * img_width, y * img_height) \n",
    "                for x, y in zip(polygon_norm[::2], polygon_norm[1::2])\n",
    "            ]\n",
    "            \n",
    "            annotations.append({\n",
    "                'class_id': class_id,\n",
    "                'polygon': polygon_abs\n",
    "            })\n",
    "\n",
    "print(\"Annotations for selected image:\")\n",
    "print(annotations)\n",
    "\n",
    "# Plot the image with polygons\n",
    "fig, ax = plt.subplots(1, figsize=(12, 8))\n",
    "ax.imshow(image)\n",
    "\n",
    "for ann in annotations:\n",
    "    polygon = ann['polygon']\n",
    "    class_id = ann['class_id']\n",
    "    label = categories[class_id] if categories else str(class_id)\n",
    "    \n",
    "    # Draw polygon\n",
    "    poly_patch = patches.Polygon(\n",
    "        polygon,\n",
    "        closed=True,\n",
    "        linewidth=2,\n",
    "        edgecolor='red',\n",
    "        facecolor='none'\n",
    "    )\n",
    "    ax.add_patch(poly_patch)\n",
    "    \n",
    "    # Add label at the top-left corner of the polygon's bounding box\n",
    "    polygon_np = np.array(polygon)\n",
    "    x_min, y_min = polygon_np.min(axis=0)\n",
    "    plt.text(\n",
    "        x_min, y_min - 5,\n",
    "        label,\n",
    "        color='red',\n",
    "        fontsize=12,\n",
    "        bbox=dict(facecolor='white', alpha=0.5, edgecolor='none')\n",
    "    )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Circuit Diagram Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CircuitDiagramDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, transform=None, img_size=224):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.transform = transform\n",
    "        self.img_size = img_size\n",
    "        \n",
    "        # Get list of image files\n",
    "        self.image_files = [f for f in os.listdir(image_dir) \n",
    "                          if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        original_width, original_height = image.size\n",
    "        \n",
    "        # Load corresponding YOLO annotations\n",
    "        label_name = os.path.splitext(img_name)[0] + \".txt\"\n",
    "        label_path = os.path.join(self.label_dir, label_name)\n",
    "        \n",
    "        boxes = []\n",
    "        labels = []\n",
    "        \n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) < 9:  # At least 1 class + 4 points (8 coordinates)\n",
    "                        continue\n",
    "                    \n",
    "                    # Parse YOLO polygon format\n",
    "                    class_id = int(parts[0])\n",
    "                    polygon_norm = list(map(float, parts[1:]))\n",
    "                    \n",
    "                    # Convert normalized polygon to absolute coordinates\n",
    "                    polygon_abs = [\n",
    "                        (x * original_width, y * original_height) \n",
    "                        for x, y in zip(polygon_norm[::2], polygon_norm[1::2])\n",
    "                    ]\n",
    "                    \n",
    "                    # Convert polygon to bounding box\n",
    "                    polygon_np = np.array(polygon_abs)\n",
    "                    x_min, y_min = polygon_np.min(axis=0)\n",
    "                    x_max, y_max = polygon_np.max(axis=0)\n",
    "                    \n",
    "                    # Convert to [x_min, y_min, x_max, y_max] format\n",
    "                    boxes.append([x_min, y_min, x_max, y_max])\n",
    "                    labels.append(class_id)\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        # Scale boxes to target image size\n",
    "        scale_x = self.img_size / original_width\n",
    "        scale_y = self.img_size / original_height\n",
    "        \n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
    "        if boxes.nelement() != 0:  # Check if boxes is not empty\n",
    "            boxes[:, 0::2] *= scale_x  # x coordinates\n",
    "            boxes[:, 1::2] *= scale_y  # y coordinates\n",
    "\n",
    "        labels = torch.tensor(labels, dtype=torch.int64)\n",
    "        \n",
    "        # Create target dictionary\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"image_id\": torch.tensor([idx])\n",
    "        }\n",
    "        \n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "dataset = CircuitDiagramDataset(\n",
    "    image_dir=\"images/train\",\n",
    "    label_dir=\"images/labels\",\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "def get_model(num_classes=40):\n",
    "    \"\"\"\n",
    "    Create a Faster R-CNN model with a ResNet-50-FPN backbone\n",
    "    Args:\n",
    "        num_classes (int): Number of object classes (excluding background)\n",
    "    \"\"\"\n",
    "    # 1. Load pre-trained base model\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(\n",
    "        weights='DEFAULT'  # Uses pre-trained weights\n",
    "    )\n",
    "    \n",
    "    # 2. Modify the classifier head\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    \n",
    "    # Important: num_classes + 1 (for background class)\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes + 1)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],  # ImageNet stats\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize dataset and dataloaders\n",
    "train_dataset = CircuitDiagramDataset(\"images/train\", \"labels/train\", transform=transform)\n",
    "val_dataset = CircuitDiagramDataset(\"images/val\", \"labels/val\", transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "# Initialize model\n",
    "num_classes = 40  # Adjust based on dataset\n",
    "model = get_model(num_classes)\n",
    "device = \"mps\"\n",
    "model.to(device)\n",
    "\n",
    "# Define optimizer and learning rate scheduler\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.AdamW(params, lr=1e-4, weight_decay=1e-4)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, train_loader, val_loader, device, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for images, targets in train_loader:\n",
    "            images = list(img.to(device) for img in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss_dict = model(images, targets)\n",
    "            loss = sum(loss for loss in loss_dict.values())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0\n",
    "            for images, targets in val_loader:\n",
    "                images = list(img.to(device) for img in images)\n",
    "                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "                loss_dict = model(images, targets)\n",
    "                loss = sum(loss for loss in loss_dict.values())\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        lr_scheduler.step()\n",
    "        print(f\"Epoch {epoch+1}, Train Loss: {total_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    print(\"Training Complete\")\n",
    "\n",
    "train_model(model, train_loader, val_loader, device, num_epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.80 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.23 ðŸš€ Python-3.12.6 torch-2.5.1 MPS (Apple M2)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolo11n.pt, data=./data.yaml, epochs=10, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=mps, workers=8, project=None, name=train32, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=/Users/enerinn/Developer/CS25-338-main/runs/detect/train32\n",
      "Overriding model.yaml nc=80 with nc=47\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    439837  ultralytics.nn.modules.head.Detect           [47, [64, 128, 256]]          \n",
      "YOLO11n summary: 319 layers, 2,599,005 parameters, 2,598,989 gradients, 6.5 GFLOPs\n",
      "\n",
      "Transferred 448/499 items from pretrained weights\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/enerinn/Developer/CS25-338-main/Model/labels/train... 4216 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4216/4216 [00:01<00:00, 3707.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /Users/enerinn/Developer/CS25-338-main/Model/labels/train.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "\u001b[34m\u001b[1mval: \u001b[0mError loading data from /Users/enerinn/Developer/CS25-338-main/Model/images/val\nSee https://docs.ultralytics.com/datasets for dataset formatting guidance.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/Developer/CS25-338-main/Model/.venv/lib/python3.12/site-packages/ultralytics/data/base.py:125\u001b[0m, in \u001b[0;36mBaseDataset.get_img_files\u001b[0;34m(self, img_path)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;66;03m# self.img_files = sorted([x for x in f if x.suffix[1:].lower() in IMG_FORMATS])  # pathlib\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m im_files, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mNo images found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFORMATS_HELP_MSG\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mAssertionError\u001b[0m: \u001b[34m\u001b[1mval: \u001b[0mNo images found in /Users/enerinn/Developer/CS25-338-main/Model/images/val. Supported formats are:\nimages: {'tiff', 'jpeg', 'pfm', 'dng', 'bmp', 'tif', 'mpo', 'webp', 'png', 'jpg', 'heic'}\nvideos: {'webm', 'm4v', 'avi', 'mp4', 'wmv', 'mkv', 'mpeg', 'mov', 'ts', 'asf', 'gif', 'mpg'}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m YOLO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myolo11n.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Use 'yolov8s', 'yolov8m', etc., for larger models\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./data.yaml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m metrics \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mval()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(metrics\u001b[38;5;241m.\u001b[39mbox\u001b[38;5;241m.\u001b[39mmap)  \u001b[38;5;66;03m# Check mAP score\u001b[39;00m\n",
      "File \u001b[0;32m~/Developer/CS25-338-main/Model/.venv/lib/python3.12/site-packages/ultralytics/engine/model.py:802\u001b[0m, in \u001b[0;36mModel.train\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m    801\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mhub_session \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession  \u001b[38;5;66;03m# attach optional HUB session\u001b[39;00m\n\u001b[0;32m--> 802\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m}:\n",
      "File \u001b[0;32m~/Developer/CS25-338-main/Model/.venv/lib/python3.12/site-packages/ultralytics/engine/trainer.py:207\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 207\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/CS25-338-main/Model/.venv/lib/python3.12/site-packages/ultralytics/engine/trainer.py:327\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[0;34m(self, world_size)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m world_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_ddp(world_size)\n\u001b[0;32m--> 327\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m nb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader)  \u001b[38;5;66;03m# number of batches\u001b[39;00m\n\u001b[1;32m    330\u001b[0m nw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mround\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mwarmup_epochs \u001b[38;5;241m*\u001b[39m nb), \u001b[38;5;241m100\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mwarmup_epochs \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# warmup iterations\u001b[39;00m\n",
      "File \u001b[0;32m~/Developer/CS25-338-main/Model/.venv/lib/python3.12/site-packages/ultralytics/engine/trainer.py:294\u001b[0m, in \u001b[0;36mBaseTrainer._setup_train\u001b[0;34m(self, world_size)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_dataloader(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, rank\u001b[38;5;241m=\u001b[39mLOCAL_RANK, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m}:\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;66;03m# Note: When training DOTA dataset, double batch size could get OOM on images with >2000 objects.\u001b[39;00m\n\u001b[0;32m--> 294\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_loader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dataloader\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtestset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mobb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mval\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_validator()\n\u001b[1;32m    298\u001b[0m     metric_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidator\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mkeys \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_loss_items(prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Developer/CS25-338-main/Model/.venv/lib/python3.12/site-packages/ultralytics/models/yolo/detect/train.py:49\u001b[0m, in \u001b[0;36mDetectionTrainer.get_dataloader\u001b[0;34m(self, dataset_path, batch_size, rank, mode)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m}, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMode must be \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch_distributed_zero_first(rank):  \u001b[38;5;66;03m# init dataset *.cache only once if DDP\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m shuffle \u001b[38;5;241m=\u001b[39m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(dataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrect\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m shuffle:\n",
      "File \u001b[0;32m~/Developer/CS25-338-main/Model/.venv/lib/python3.12/site-packages/ultralytics/models/yolo/detect/train.py:43\u001b[0m, in \u001b[0;36mDetectionTrainer.build_dataset\u001b[0;34m(self, img_path, mode, batch)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03mBuild YOLO Dataset.\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m    batch (int, optional): Size of batches, this is for `rect`. Defaults to None.\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     42\u001b[0m gs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mint\u001b[39m(de_parallel(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\u001b[38;5;241m.\u001b[39mstride\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m32\u001b[39m)\n\u001b[0;32m---> 43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuild_yolo_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mval\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/CS25-338-main/Model/.venv/lib/python3.12/site-packages/ultralytics/data/build.py:87\u001b[0m, in \u001b[0;36mbuild_yolo_dataset\u001b[0;34m(cfg, img_path, batch, data, mode, rect, stride, multi_modal)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Build YOLO Dataset.\"\"\"\u001b[39;00m\n\u001b[1;32m     86\u001b[0m dataset \u001b[38;5;241m=\u001b[39m YOLOMultiModalDataset \u001b[38;5;28;01mif\u001b[39;00m multi_modal \u001b[38;5;28;01melse\u001b[39;00m YOLODataset\n\u001b[0;32m---> 87\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimg_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimgsz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# augmentation\u001b[39;49;00m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# TODO: probably add a get_hyps_from_cfg function\u001b[39;49;00m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrect\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# rectangular batches\u001b[39;49;00m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43msingle_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msingle_cls\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolorstr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmode\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfraction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfraction\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/CS25-338-main/Model/.venv/lib/python3.12/site-packages/ultralytics/data/dataset.py:64\u001b[0m, in \u001b[0;36mYOLODataset.__init__\u001b[0;34m(self, data, task, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_segments \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_keypoints), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan not use both segments and keypoints.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 64\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/CS25-338-main/Model/.venv/lib/python3.12/site-packages/ultralytics/data/base.py:73\u001b[0m, in \u001b[0;36mBaseDataset.__init__\u001b[0;34m(self, img_path, imgsz, cache, augment, hyp, prefix, rect, batch_size, stride, pad, single_cls, classes, fraction)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprefix \u001b[38;5;241m=\u001b[39m prefix\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfraction \u001b[38;5;241m=\u001b[39m fraction\n\u001b[0;32m---> 73\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mim_files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_img_files\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_labels()\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_labels(include_class\u001b[38;5;241m=\u001b[39mclasses)  \u001b[38;5;66;03m# single_cls and include_class\u001b[39;00m\n",
      "File \u001b[0;32m~/Developer/CS25-338-main/Model/.venv/lib/python3.12/site-packages/ultralytics/data/base.py:127\u001b[0m, in \u001b[0;36mBaseDataset.get_img_files\u001b[0;34m(self, img_path)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m im_files, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mNo images found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFORMATS_HELP_MSG\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mError loading data from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mHELP_URL\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfraction \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    129\u001b[0m     im_files \u001b[38;5;241m=\u001b[39m im_files[: \u001b[38;5;28mround\u001b[39m(\u001b[38;5;28mlen\u001b[39m(im_files) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfraction)]  \u001b[38;5;66;03m# retain a fraction of the dataset\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: \u001b[34m\u001b[1mval: \u001b[0mError loading data from /Users/enerinn/Developer/CS25-338-main/Model/images/val\nSee https://docs.ultralytics.com/datasets for dataset formatting guidance."
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a pre-trained YOLO model\n",
    "model = YOLO(\"yolo11n.pt\")  # Use 'yolov8s', 'yolov8m', etc., for larger models\n",
    "\n",
    "# Train the model\n",
    "results = model.train(data=\"./data.yaml\", epochs=10, device=\"mps\")\n",
    "\n",
    "\n",
    "metrics = model.val()\n",
    "print(metrics.box.map)  # Check mAP score\n",
    "results.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
